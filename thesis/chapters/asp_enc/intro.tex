\chapter{ASP encoding of BNN robustness}

In this section I show possible encodings of binarised neural networks
and of the robustness problem. Specifically, I create an encoding of a binarised
neural network using negation as failure, input regions and adversarial inputs.
% TODO: I also discuss possible encoding of a binarised neural network
% with the use of classical negation. I compare this encoding to the one
% using negation as failure from the standpoint of compotational time.
% TODO: Further I discuss the possibility of encoding any neural network
% with weights from rational numbers and step function as activation function.
Finally, I create a Python program able to encode weights and biases of
a binarised neural network into a part of Clingo program.

% Notes on the linearity of BNN
%     The next layer is only dependent on the previous layer

% Implementation of binarised perceptron
%     Difference in grounding for inequality vs predicate with variable
% Implementation of argmax layer
%     Difference in grounding for inequality vs predicate with variable

% Implementation of robustness
%     Specifying the input space
%     Specifying the adversarial inputs

% Adding classical negation into the mix
% Possible extension of this model over rational numbers

% Encoding verification problem into Clingo language using Python
