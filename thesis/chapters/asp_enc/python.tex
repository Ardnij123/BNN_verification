\section{Encoding robustness of BNN into logic program}

For transcription of the binarised neural network and the robustness problem,
I use Python script. This script takes on input a binarised neural network in form of
a directory of csv files, a base input in form of text file and possibly multiple
other parameters (see~\Cref{sec:python_pars}). It then encodes this model into
logic program using specified parameters. Finally, it executes the logic program
using Clingo, counting all models with output that differs from the desired one.

\subsection{Structure of saved BNN}

The script allows for evaluation of binarised neural networks in the form of
binarised multi-layer perceptron with batch normalization.

Binarised neural network is represented as a directory containing
a subdirectory named \texttt{blk$X$} for every inner layer $X$ of the network
and a subdirectory named \texttt{out\_blk} for the output Argmax layer.
Each inner block subdirectory then contain files \texttt{bn\_bias.csv},
\texttt{bn\_mean.csv}, \texttt{bn\_var.csv}, \texttt{bn\_weight.csv},
\texttt{lin\_bias.csv} and \texttt{lin\_weight.csv}. Each of these files contain
vector or matrix corresponding to the parameter of the layer.
The last Argmax layer subdirectory contains only files \texttt{lin\_bias.csv}
and \texttt{lin\_weight.csv}.

Further follows specification of individual files.

\subsubsection{\texttt{bn\_bias.csv}}

File contains a vector specified using a single line of comma separated values.
This vector corresponds to the biases $\vec \gamma$ of a binarised single-layer
perceptron with batch normalization, $i$-th entry corresponds to the bias of perceptron on
$i$-th position of this layer.

\subsubsection{\texttt{bn\_mean.csv}}

File contains a vector specified using a single line of comma separated values.
This vector corresponds to the means $\vec \mu$ of a binarised single-layer
perceptron with batch normalization, $i$-th entry corresponds to the mean of perceptron on
$i$-th position of this layer.

\subsubsection{\texttt{bn\_var.csv}}

File contains a vector specified using a single line of comma separated values.
This vector corresponds to the variances $\vec \sigma^2$ of a binarised single-layer
perceptron with batch normalization. To get the standard deviations $\vec \sigma$,
piecewise square root must be applied to this vector. $i$-th entry corresponds to
the variance of perceptron on $i$-th position of this layer.

\subsubsection{\texttt{bn\_weight.csv}}

File contains a vector specified using a single line of comma separated values.
This vector corresponds to the weights $\vec \alpha$ of a binarised single-layer
perceptron with batch normalization, $i$-th entry corresponds to the weight of perceptron on
$i$-th position of this layer.

\subsubsection{\texttt{lin\_bias.csv}}

File contains a vector specified using a single line of comma separated values.
This vector corresponds to the biases $\vec b$ of a binarised single-layer
perceptron with batch normalization, $i$-th entry corresponds to bias the perceptron on
$i$-th position of this layer.

\subsubsection{\texttt{lin\_weight.csv}}

File contains a matrix specified using a table of values with comma as separator of columns.
This matrix corresponds to the weights $\mathbf{w}$ of a binarised single-layer
perceptron with batch normalization. Values of this matrix are $\pm 1$-binarised.
Vector constituted by the $i$-th row of the matrix $\mathbf{w}$ corresponds to
the vector of weights from previous layer to the perceptron on $i$-th position
of this layer.
Entry on $i$-th row and $j$-th column corresponds to the weight from $j$-th position
of the previous layer to $i$-th position of this layer.

\subsection{Transformation of BNN}

After loading, the binarised neural network with batch normalization
(\Cref{def:perceptron_bn,def:argmax}) is transformed into the whole number-valued
form using Python package Numpy~\cite{harris2020array}.

Further I show the whole transformation with representation using matrices and vectors
contrary to vectors and scalar values that were used so far.
When using vectors I assume they represented by a column of values.
In the implementation, they are transposed when loading from files.
To distinguish between them, I will use $\cdot$ for dot product,
$\star$ for pointwise product and $\div$ for pointwise division.

\subsubsection{Transformation of inner layer}

Starting with binarised single-layer with batch normalization~\cref{def:perceptron_bn}:
\[t^{\BB}(\vec x) \equiv \vec\alpha \star \left({((\vec b + \mathbf{w} \cdot \vec x) - \vec\mu) \div \vec\sigma}\right) + \vec\gamma \geq 0\]
Using~\cref{lem:batch_perceptron}, the expression can be altered to use only a single
bias and a matrix of weights.
\[b_i' = \begin{cases}
	b_i - \mu_i + {\sigma_i\over \alpha_i}\cdot \gamma_i & {\alpha_i \over \sigma_i} > 0\\
	\gamma_i & {\alpha_i\over \sigma_i} = 0\\
	-b_i + \mu_i - {\sigma_i\over \alpha_i}\cdot \gamma_i & {\alpha_i \over \sigma_i} < 0\\
\end{cases}\]
\[\vec{w_i}' = \begin{cases}
	\vec{w_i} & {\alpha_i \over \sigma_i} > 0\\
	\vec{0} & {\alpha_i\over \sigma_i} = 0\\
	-\vec{w_i} & {\alpha_i \over \sigma_i} < 0\\
\end{cases}\]
\[t^\BB(\vec x) \equiv \vec b' + \mathbf{w}'\cdot \vec x \geq 0\]
In the encoding of weight, there is now a possibility for some rows to be vectors of zeros.
That is not a problem as they can still be encoded into the Clingo language.
In aggregate expressions any whole numbers may be used.

To further simplify the encoding into logic program, the $\pm 1$-binarised
input vector $\vec x$ may be mapped to vector $\vec x_b$
of values $\{1, 0\}$~(\Cref{sec:01-mapping}).
\[t^\BB(\vec x) \equiv \vec b' + \mathbf{w}'\cdot (2\vec x_b - \vec 1) \geq 0\]
\[\vec b'' = \frac{\vec b' - \mathbf{w}'\cdot \vec 1}{2}\]
\[t^\BB(\vec x) \equiv \vec b'' + \mathbf{w}'\cdot \vec x_b \geq 0\]
Finally to prepare the perceptron for the transcription to logic program,
pointwise flooring function may be used on the bias $\vec b''$.
If the use of $\{1, 0\}$-binarised perceptron is not desired, $\vec b'$ and $\vec x$
can be used in place of $\vec b''$ and $\vec x_b$ in this step.
\[t^\BB(\vec x) \equiv \floor{\vec b''} + \mathbf{w}'\cdot \vec x_b \geq 0\]

\subsubsection{Transformation of Argmax layer}\label{sec:pyt_argmax}

The transformation of Argmax layer is arguably simpler than the one of
inner layers. It starts directly with inner potential and outputs vector
with 1 only at the position of largest value.
\[t^{am}(\vec x) = \argmax(\vec b + \mathbf{w}\cdot \vec x)\]
Again as shown in~\Cref{sec:01-mapping,sec:01-argmax}, the input vector $\vec x$
may be mapped to vector $\vec x_b$ of values $\{1,0\}$ to simplify the encoding
of logic program.
\[t^{am}(\vec x) = \argmax(\vec b + \mathbf{w}\cdot (2\vec x_b - \vec 1))\]
\[\vec b' = \frac{\vec b - \mathbf{w}\cdot \vec 1}{2}\]
\[t^{am}(\vec x) = \argmax(\vec b' + \mathbf{w}\cdot \vec x_b)\]
As shown in~\cref{lem:integer_argmax}, the bias may be split to its integer part
and fractional part and ordering made on fractional part. This ordering should have
the position with highest fractional part as highest priority. The algorithm for the
sorting needs to be stable as in the event of the fractional part being the same value,
lower position has higher priority.
\[\vec b' = \floor{\vec b'} + \{\vec b'\}\]
\[ord = \argsort(-\{\vec b'\})\]

\subsection{Parameters of the Evaluator}\label{sec:python_pars}

% TODO
