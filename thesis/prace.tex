\documentclass{fithesis}

\thesissetup{%
    gender=m,
	author=Jindřich Matuška,
	id=525183,
	type=bc,
	university=mu,
	faculty=fi,
	locale=english,
	department=Department of Computer Systems and Communications,
	date=2024/05/22,
	place=Brno,
	title=Verification of binarised neural networks using ASP,
	keywords={bnn, verification, binarized neural networks
              answer set programming, asp},  % TODO
	abstract={foo bar},  % TODO
	advisor={RNDr. Samuel Pastva, PhD.},
	thanks={
        Computational resources were provided by the e-INFRA CZ project (ID:90254),
        supported by the Ministry of Education, Youth and Sports of the Czech Republic.
    }, % TODO
    bib=bibliography.bib,
}

\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{listings}

% Landscape fullpage tables
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage{geometry}

\input{header.tex}

\begin{document}
\chapter{Introduction}

% - Co řešíme, proč to řešíme
% 	+ NP-úplné problémy
% 		* zde bychom mohli verifikovat ex-post
% 	+ problémy, u kterých neznáme řešení
% 		* případně verifikovat řešení může být samo exponenciálně náročné
% 	+ kritické problémy
% 		* při špatném výsledku nás to může stát hodně životů/peněz
% - aktuální stav verifikace BNN a neuronek obecně (SAT/SMT)
% - práce související s tématem (state of the art)
% - Existuje nějaký framework?
% - jaké jsou aktuální limity pro velikosti verifikovaných sítí
% - možná rozdělit na 2 kapitoly?

Deep neural networks (DNN) are state-of-the-art technology. They are used in many
real-world applications e.g.\ medicine, self-driving cars, autonomous systems,
many of which are critical. % TODO: is this the right meaning?
Deep neural networks used in natural language processing have up to hundreds of
billions of parameters~\cite{2021arXiv210901652W}.
That is too many for people to comprehend. Tools for the automation
of DNN verification are needed.

Verification of neural networks is split into two categories. First,
qualitative verification, searches through input space looking for any
adversarial input, e.g.\ an input which results in wrong output.
Second, quantitative verification, searches through input space
determining the size part of the input space giving adversarial outputs.

For the qualitative disproving of the verity of general DNNs,
many algorithms for finding adversarial
inputs have already been developed~\cite{7958570, chen2020real, 8578273}.
The DNN can be qualitatively disproven by finding any adversarial input.
Proving the nonexistence of adversarial input is usually made by generating constraints
on adversarial inputs and then showing there can be no such
input~\cite{Isac2022NeuralNV}. This is computationally much more difficult
and prone to errors emerging from inaccuracies of floating point numbers operations.
It is often made with the use of a simplex algorithm combined with 
the satisfiability modulo theories (SMT) paradigm~\cite{Isac2022NeuralNV, marabou2019, reluplex2017}.
As far as I know, there is no reasonably quick quantitative
validator of general DNNs yet.

As DNNs generally base their computations on floating point numbers,
the computation of output is hard. For this, quantization,
a new branch of DNN development, where instead of 32 or 64-bit long floating
point numbers, low-bit-width (eg. 4-bit) fixed point numbers are used.
This mitigates the high cost of computation and the need for high-end devices.
Extreme examples of quantization are Binary neural networks (BNN).
These use binary parameters for their computations, resulting in much
cheaper computational price, both when learning and in production,
while maintaining near state-of-the-art results~\cite{Agrawal2023}.

While both qualitative and quantitative BNN verificators exist,
they scale only up to millions of parameters for qualitative verification~\cite{10.1007/978-3-030-03592-1_16}
and tens of thousands of parameters for quantitative verification~\cite{10.1145/3563212}.
These verificators can even compute on natural numbers only, without errors.

In this thesis, I have implemented a quantitative BNN validator using the ASP paradigm
in the framework Clingo from Potassco. I~have also evaluated the speed of this
validator against the state-of-the-art implementation BNNQuanalyst~\cite{10.1145/3563212}
on MNIST~\cite{mnist2017} and Fashion MNIST~\cite{xiao2017fashion} datasets.

% TODO: sth about results

\chapter{Used tools}

\section{Answer set programming}

Answer set programming (ASP) is a form of declarative
programming oriented towards difficult, primarily NP-hard,
search problems~\cite{lifschitz2008answer}.
ASP is particularly suited for solving difficult combinatorial search problems%
~\cite{anger2005glimpse}.
ASP is somewhat closely related to propositional
satisfability checking (SAT)
in sense that the problem is represented as logic program. Difference is in
computational mechanism of finding solution.

\subsection{Syntax and semantics of ASP}

Logic program $\Pi$ in ASP is a set of rules consisting of atoms.
An atom is the elementary construct for representing knowledge~\cite{Delgrande}.
Each atom constitutes a single variable which either is or is not in the answer
set (solution). An atom can be seen as a possible feature of the answer set.
Each rule $r_i\in \Pi$ has form of
\begin{equation}
    \asprule{\asphead(r_i)}{\aspbody(r_i)}
\end{equation}
$\asphead(r_i)$ is then a single atom $p_0$, while $\aspbody(r_i)$ is a set of zero
or more atoms $\{p_1, \dots, p_m, \aspnot p_{m+1}, \dots \aspnot p_n\}$.
\begin{equation}
    \asprule{p_0}{p_1, \dots, p_m, \aspnot p_{m+1}, \dots, \aspnot p_n}
\end{equation}
Further the $\aspbody(r_i)$ can be split between
$\aspbody^+(r_i) = \{p_1, \dots, p_m\}$
and $\aspbody^-(r_i) = \{p_{m+1}\dots p_n\}$.
If $\forall r_i \in \Pi.\, \aspbody^-(r_i) = \emptyset$, then $\Pi$
is called \textit{basic}.

Semantically rule (2.2) means \textit{If all atoms from $\aspbody^+(r_i)$
are included in answer set and no atom from $\aspbody^-(r_i)$ is in answer set,
then $\asphead(r_i)$ has to be in the set.}

Set of atoms $X$ is closed under a basic program $\Pi$ if for any
$r_i\in \Pi.\,\aspbody(r_i)\subseteq X \implies \asphead(r_i)\in X$.
For general case the concept of \textit{reduct of a program $\Pi$ relative
to a set $X$ of atoms} is needed.
\begin{equation}
    \Pi^X = \{\asphead(r_i)\leftarrow\aspbody^+(r_i)
              \mid r_i\in \Pi,\,\aspbody^-(r_i)\cap X = \emptyset\}
\end{equation}
This program is always basic as it only contains positive atoms.

Let's denote $\mathrm{Cn}(\Pi)$ the minimal set of atoms closed under
a basic program $\Pi$, that is
\begin{equation}
    \mathrm{Cn}(\Pi) \supseteq
    \{\asphead(r_i) \mid r_i\in \Pi, \aspbody(r_i) \subseteq \mathrm{Cn}(\Pi)\}
\end{equation}
% TODO: find article that proves there is only single such minimal set
Such set is said to constitute program $\Pi$.
Set $X$ of atoms is said to be answer set of $\Pi$ iff
\begin{equation}
    \mathrm{Cn}(\Pi^X) = X
\end{equation}
In other words, the answer set is a model of $\Pi$ such that its every atom
is grounded in $\Pi$.

Let's illustrate this property on an example.
\begin{equation}
    \Pi = \{\asprule{p}{p}, \asprule{q}{\aspnot p}\}
\end{equation}
There are 4 subsets of set of all atoms. First the reduct relative to
the subset is made, on it the calculation of constituting set is made.
If $X = \mathrm{Cn}(\Pi^X)$, then $X$ is one of answer sets.
\begin{center}
    \begin{tabular}{L L L}\toprule{}%
        X        & \Pi^X          & \mathrm{Cn} (\Pi^X) \\\midrule{}%
        \{\}     & \asprule{p}{p} & \{q\} \\
                 & \asprule{q}{}  &       \\\addlinespace[0.5em]
        \{p\}    & \asprule{p}{p} & \{\}  \\\addlinespace[0.5em]
        \{q\}    & \asprule{p}{p} & \{q\} \\
                 & \asprule{q}{}  &       \\\addlinespace[0.5em]
        \{p, q\} & \asprule{p}{p} & \{\}  \\
        \bottomrule{}
    \end{tabular}
\end{center}
There is only a single answer set of $\Pi$, that is $\{q\}$. Also, there is
an interesting type of rule in the table, $\asprule{p}{}$. This type of rule
is commonly reffered as \textit{fact} and ensures that atom $p$ is always
included in the answer set.

Another toy example is $\Pi = \{\asprule{p}{\aspnot q}, \asprule{q}{\aspnot p}\}$.
Again, there are 4 subsets of set of all atoms.
\begin{center}
    \begin{tabular}{L L L}\toprule{}%
        X        & \Pi^X         & \mathrm{Cn} (\Pi^X) \\\midrule{}%
        \{\}     & \asprule{p}{} & \{p, q\} \\
                 & \asprule{q}{} &          \\\addlinespace[0.5em]
        \{p\}    & \asprule{p}{} & \{p\}    \\\addlinespace[0.5em]
        \{q\}    & \asprule{q}{} & \{q\}    \\\addlinespace[0.5em]
        \{p, q\} &               & \{\}     \\
        \bottomrule{}
    \end{tabular}
\end{center}
This time there are two answer sets of $\Pi$, $\{p\}$ and $\{q\}$.
As we can see, the double not forms a XOR --- exactly one of atoms $p, q$ has to
be in the answer set.

\subsection{Language extensions}

Writing logic programs only using rules of form (2.2) would be very hard.
For this, many language extensions have been developed to shorten programs,
simplifying both readability and computation of stable models.
% TODO: Is this true? Do the frameworks use these?

In this section I will only show the constraint, as it can be very easily
transformed into base ASP rule. I will describe more language extensions
provided with Clingo framework I will describe in section (2.2).

\subsubsection{Constraint}

\begin{equation*}
    \asprule{}{p_1, \dots, p_m, \aspnot p_{m+1}, \dots, \aspnot p_n}
\end{equation*}
Constraint is a type of language extension with semantics that its body can
not be in the answer set. It can be rewritten as following rule~\cite{anger2005glimpse}:
\begin{equation*}
    \asprule{f}{\aspnot f, p_1, \dots, p_m, \aspnot p_{m+1}, \dots, \aspnot p_n}
\end{equation*}
where $f$ is a new atom (atom not used anywhere else).

% TODO: format this

Let's consider $\Pi$, $\asprule{f}{\aspnot f, p_1, \dots, p_m,
\aspnot p_{m+1}, \dots, \aspnot p_n} = r \in \Pi$. Let $X$ be
any set of atoms s.t. $\{p_1, \dots, p_m\}\subseteq X,
\{p_{m+1}, \dots, p_n\}\cap X = \emptyset$. If $f\in X$, then the rule
$\asprule{f}{\aspbody^+(r)}$ is not in $\Pi^x$, thus $f\notin \aspCn{\Pi^X}$
and $X \neq \aspCn{\Pi^X}$. If $f\notin X$, then the rule
$\asprule{f}{\aspbody^+(r)}$ is in $\Pi^X$, thus $f\in \aspCn{\Pi^X}$
and $X\neq \aspCn{\Pi^X}$. This shows that no set consistent with constraint
rule can be answer set.

If $X$ is not consistent with rule $r$ and $f\notin X$,
then $X$ contains some negative literal
of $r$ ($\exists x\in X.\, x\in \aspbody^-(r)$) and the rule $r$ is not in the
reduct $\Pi^X$, or $X$ does not contain some positive literal of $r$, thus the
rule $r$ is not applied. Either way, $f\notin X, f\notin \aspCn{\Pi^X}$.

Such rule is usefull for constraining the answer set. Lets assume expansion
of example from the previous section.
\begin{equation*}
    \Pi = \{
        \asprule{p_1}{\aspnot q_1},\, \asprule{q_1}{\aspnot p_1},\,
        \asprule{p_2}{\aspnot q_2},\, \asprule{q_2}{\aspnot p_2}
    \}
\end{equation*}
By adding the constraint $\asprule{}{p_1, p_2}$, any set that contains both
$p_1$ and $p_2$ is not an answer set.

\section{Clingo framework}

Clingo is an integrated ASP system, consisting of a grounder Gringo and solver
Clasp~\cite{aspEasy2016}. In the following section I will show basics of the
clingo language, gringo as translation from clingo to aspif language and
solving with clasp.

\subsection{Clingo language}
Clingo language is used for transcribing ASP programs and their extended versions.
There are 3 possible forms of rules as shown in the table bellow.
{\setlength{\tabcolsep}{0.2em}%
\begin{center}
    \begin{tabular}{l @{\hskip 1em} L l L @{\hskip 1em} L}\toprule{}%
        Type & \multicolumn{3}{l}{Form} & \mathrm{ASP\ rule}\\\midrule{}%
        Fact & \asphead(r). & & & \asprule{\asphead(r)}{}\\
        Rule & \asphead(r) &:- & \aspbody(r). & \asprule{\asphead(r)}{\aspbody(r)}\\
        Constraint & &:- & \aspbody(r). & \asprule{\phantom{\asphead(r)}}{\aspbody(r)}\\
        \bottomrule{}
    \end{tabular}
\end{center}}\noindent
Semantically these rules mean to use head of rule if whole body is of rule
is consistent with the answer set. Fact does has an empty body, thus its
head is used always. No answer set is consistent with body of constraint.

\subsubsection{Example}
Program (2.6) can be rewritten into clingo language as following
program.
\lstclingo{example.lp}
Each rule consists of head and body, separated by \texttt{:-} operator.
Every atom starts with small letter of english alphabet.
Every statements ends with a dot.

Clingo language also allows for the use of variables and arithmetic expressions
in the parameters of atoms. Variable always starts with a big letter of english
alphabet. Take the following program as an example.
% TODO: Maybe show it looks after substitution on the right?
\lstclingo{example_variables.lp}
This program contains four facts on line 1 and single substituable rule on line 2.
As can be seen on line 1, multiple statements can share single line as long as
they are all formed correctly. In the same manner, a statement can span over
multiple lines.

If the exact parameter is not needed, one can use a wildcard in the place of
the parameter in the body of rule. The wildcard behaves as anonymous variable.
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
a(1..3, 0..1).
b(X) :- a(X, _).
\end{lstlisting}

\subsubsection{Conditional literals}
% TODO section

\subsubsection{Intervals, pooling, wild cards}

Another usefull constructs in clingo language are intervals and pooling.
Consider the following example of intervals (programs are equivalent):

\begin{minipage}[t]{0.45\linewidth}%
\centering%
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
a(1..3, 0..1).
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=right, countblanklines=false]
a(1, 0). a(2, 0). a(3, 0).
a(1, 1). a(2, 1). a(3, 1).
\end{lstlisting}
\end{minipage}
Each combination of intervals is evaluated as a single rule. The first
parameter \texttt{1..3} gives 3 choices, the second \texttt{0..1} 2 choices,
thus $6=3*2$ rules can be derived.

For a more complex example consider the following program.
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
comp(X*Y) :- X = 2..20, Y = 2..20, X*Y <= 20.
prime(X) :- not comp(X), X = 2..20.
\end{lstlisting}
This program calculates prime numbers. A number is composite if it can be
written as a product of two numbers greater than 1. Else it is prime.

Pooling is very similiar to intervals. It also allows for compact writing
of rules. As intervals allows for writing sets of consecutive numbers,
pooling allows for writing any set. Using pooling, previous example could
be rewritten as following program.
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
a((1;2;3), (0;1)).
\end{lstlisting}
In this case, each pool must be enclosed in braces, else this program would
be equivalent to:
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
a(1). a(2). a(3, 0). a(1).
\end{lstlisting}

Pooling can be also done on nonnumeric parameters.
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
a(foo; bar).
\end{lstlisting}

\subsection{Gringo}

Gringo is a software that grounds program in clingo language into the format
aspif that is readable in Clasp. Gringo first resolves every rule with variables
into (possibly multiple) variable-free rules and then changes format of
the program into Clasp-readable aspif. Gringo thus can introduce new atoms
that were not obvious from the clingo program. Specification of aspif language
is written in %[https://www.cs.uni-potsdam.de/wv/publications/DBLP_conf/iclp/GebserKKOSW16x.pdf].

Aspif (ASP intermediate format) language consists of lines. First line of file
is header of form 
{%
\newcommand{\ms}{\texttt{ }}
\begin{center}
    \texttt{asp}\ms$v_m$\ms$v_n$\ms$v_r$\ms$t_1$\ms\dots\ms$t_k$
\end{center}
}\noindent
where $v_m$, $v_n$ and $v_r$ are versions of major, minor and revision numbers
respectively and each $t_i$ is a tag. Then follow lines with statements
translated from program. For this thesis only rule and show statements
are relevant.
Last line of aspif format file is a single 0.

\subsubsection{Rule statement}

Rule statement in Aspif has form of
{%
\newcommand{\ms}{\texttt{ }}
\begin{center}
    1\ms{}$H$\ms{}$B$
\end{center}
}\noindent
in which head $H$ has form of
{%
\newcommand{\ms}{\texttt{ }}
\begin{center}
    \texttt{h}\ms\texttt{m}\ms$a_1$\ms\dots\ms$a_m$
\end{center}
}\noindent
where $\texttt{h}\in\{0, 1\}$, $\texttt{m}\geq 0$,
$\forall i\in\{1, \dots, m\}a_i\in\NN^+$. Parameter \texttt{h} determines
wherther head of this rule is disjunction (0) or choice (1), \texttt{m}
determines number of literals and $a_i$ are positive literals.
Body $B$ is called normal if it has form of
{%
\newcommand{\ms}{\texttt{ }}
\begin{center}
    0\ms\texttt{n}\ms$l_1$\ms\dots\ms$l_n$
\end{center}
}\noindent
in which case it is called normal body (literals are in conjunction).
Parameter $\texttt{n}\geq 0$ determines
the length of rule body and each $l_i$ is literal. If the literal is negative,
inversed value of its index is used.

The other type of body $B$ is called weight body. Its form is
{%
\newcommand{\ms}{\texttt{ }}
\begin{center}
    1\ms{}\texttt{l}\ms{}n\ms{}$l_1$\ms{}$w_1$\ms{}\dots\ms$l_n$\ms$w_n$
\end{center}
}\noindent
Parameter $\texttt{l}\geq 0$ determines lower bound, \texttt{n}
the length of rule body, each $l_i$ is literal and $w_i\geq 1$ its weight.
If the literal is negative, inversed value of its index is used.

\subsubsection{Show statement}

Show statement is for specification of output, they result from \texttt{\#show}
directive. Each show statement is of form
{%
\newcommand{\ms}{\texttt{ }}
\begin{center}
    4\ms{}m\ms{}s\ms{}n\ms{}$l_1$\ms{}\dots\ms$l_n$
\end{center}
}\noindent
where \texttt{m} is length of string \texttt{s}, \texttt{s} is string
with name, \texttt{n} is number length of condition and $l_i$ are literals.
If no \texttt{\#show} directive is in clingo file, all atoms are to be shown.

\subsubsection{Example}

Let's illustrate the gringo parsing on some programs%
\footnote{To prevent gringo from making optimalizations,I have run all examples
in this section with option \texttt{{-}{-}preserve-facts=all}}%
. First the program 
$\Pi = \{\asprule{p}{\aspnot q}, \asprule{q}{\aspnot p}\}$%
\footnote{In fact the aspif file generated by Gringo would have lines 2 and 3
swapped. In this thesis I have made this swap to better illustrate
the translation.}.

\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]

p :- not q.
q :- not p.
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[numbers=right, countblanklines=false]
asp 1 0 0
1 0 1 2 0 1 -1
1 0 1 1 0 1 -2
4 1 q 1 1
4 1 p 1 2
0
\end{lstlisting}
\end{minipage}

First line of aspif file is headder. Lines 1 and 2 of clingo file are
equivalent to lines 2 and 3 of aspif file. There you can see transcription
of negative literals \texttt{not q} and \texttt{not p} as -1 and -2
respectively. Let's now decompose the line 2 of aspif program.
\begin{equation*}
    \begin{array}{r rrr rrr}
        1 & 0 & 1 & 2 & 0 & 1 & -1\\
        S & h & m & a_1 & f & n & l_1\\
    \end{array}
\end{equation*}
Begining with $S=1$, this statement is a rule. Head of this rule is composed
of $h=0$, head of this rule is disjunction. As $m=1$, if body is consistent
with answer set, then the literal in the head has to be in the answer set.
Finally, the literal on $a_1=2$ is alias for \texttt{p}.
The body is then composed from $f=0$, the literals are in conjunction.
Then $n=1$ means only one literal is in the body, that is $l_1=-1$, meaning
negative literal with alias 1 (\texttt{not q}).

Next, on lines 4 and 5, there are two statements for showing atoms.
\begin{equation*}
    \begin{array}{r rr rr}
        4 & 1 & \texttt{q} & 1 & 1\\
        S & m & s & n & l_1\\
    \end{array}
\end{equation*}
First, is a show statement ($S=4$) for a single-lettered ($m=1$)
atom \texttt{q}. It printed out if single ($n=1$) literal
numbered $l_1=1$ is in the answer set. Similiarly, the single-lettered atom
\texttt{p} is printed out if single literal numbered 2 is in the answer set.

To illustrate the variable resolving, let me show you another clingo program
beeing parsed by gringo.

\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]

a(1).
a(2).
a(3).
b(2).
b(X+3) :- a(X), b(X).
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[numbers=right, countblanklines=false]
asp 1 0 0
1 0 1 1 0 0
1 0 1 2 0 0
1 0 1 3 0 0
1 0 1 4 0 0
1 0 1 5 0 2 4 2
4 4 b(2) 1 4
4 4 b(5) 1 5
4 4 a(1) 1 1
4 4 a(2) 1 2
4 4 a(3) 1 3
0
\end{lstlisting}
\end{minipage}

On lines 1--4 there are 4 facts \texttt{a(1)}, \texttt{a(2)}, \texttt{a(3)} and
\texttt{b(2)}. Each of this fact instantiates
a new atom with a single parameter. Then on line 5 is
rule with variable $X$. To resolve this rule, gringo first looks through
all known atoms of \texttt{a} and \texttt{b} and finds tuples that satisfy
the positive literals of the body of the rule. There is only a single such
instantiation, that is \texttt{a(2)} and \texttt{b(2)}. Thus new atom
\texttt{b(5)} is added to known atoms. This resolution of parametric rules
continues until there is no rule that could add any new atoms.

Now, when we know all possibly needed atoms, clingo adds rules to the program.
The 4 facts from lines 1--4 of clingo ended up as
rules on lines 2--5 of aspif. As can be seen,
each fact got rewritten as rule with no parameter, thus applying every time.
Rule from line 5 of clingo got transcribed as a single rule on line 6 in aspif
- there is only a single possible assignment of atoms that fit the rule.
Literal 5, corresponding to atom \texttt{b(5)}, is in the answer set
if both literals 4 and 2 corresponding to \texttt{b(2)} and \texttt{a(2)} resp.
are in the answer set. This rule can not be applied on any other tuple of
literal, thus the grounding of rules is over. Further in aspif file there are
only show statements.

It should be noted that the parameters of atoms are only used in the grounding
process. After that, the atoms for the solver are only annonymous numbers.

One problem gringo has is that it is prone to generating endless number
of atoms. Take the following program as an example.

\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]

a(1).
a(X+1) :- a(X).
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[numbers=right, countblanklines=false]
asp 1 0 0
1 0 1 1 0 0
1 0 1 2 0 1 1
1 0 1 3 0 1 2
1 0 1 4 0 1 3
1 0 1 5 0 1 4|\Suppressnumber|
...|\Reactivatenumber|
\end{lstlisting}
\end{minipage}
As can be seen, in this case searching for atoms ends up with new atoms
endlessly emerging. When writing clingo programs, one has to be aware of this
behaviour.

Another possibly unexcpected behaviour is that gringo works only on 32 or
64-bit integers. This is better shown on the following example.

\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]

a(1).
a(2*X) :- a(X).
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[numbers=right, countblanklines=false]
asp 1 0 0
1 0 1 1 0 0
1 0 1 2 0 1 1|\Suppressnumber|
...|\setcounter{lstnumber}{33}\Reactivatenumber|
1 0 1 33 0 1 32
4 4 a(1) 1 1
4 4 a(2) 1 2|\Suppressnumber|
...|\setcounter{lstnumber}{63}\Reactivatenumber|
4 12 a(536870912) 1 30
4 13 a(1073741824) 1 31
4 14 a(-2147483648) 1 32
4 4 a(0) 1 33
0
\end{lstlisting}
\end{minipage}
As seen on lines 65--67, the parameter of \texttt{a} overflown to negative
values and then to zero.

\subsection{Extensions in Clingo language}

Further I will show some of extensions of Clingo language usefull for
the implementation part of this thesis.

\subsubsection{Disjunctive logic programs}
Disjunctive logic programs allow of use of disjunction in the fact or rule head.
{\setlength{\tabcolsep}{0.2em}%
\begin{center}
    \begin{tabular}{l @{\hskip 1em} L l L}\toprule{}%
        Type & \multicolumn{3}{l}{Form}\\\midrule{}%
        Fact & A_0; \dots; A_m. & & \\
        Rule & A_0; \dots; A_m &:- & L_0, \dots, L_n.\\
        \bottomrule{}
    \end{tabular}
\end{center}}\noindent
where $A_0; \dots; A_m$ are atoms forming rule head. Semantically the rule means
if body holds, than at least one of $A_0, \dots, A_m$ holds. Additionaly, set
of atoms devised by this rule has to be minimal.
% TODO: wtf is minimality criterion? can it be written better?
The disjunctive logic programs are not commonly used as they are making
the computational complexity higher.
% Is this really the case?

\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]

{q}.
a; b; c :- q.
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[numbers=right, countblanklines=false]
asp 1 0 0
1 1 1 1 0 0
1 0 3 2 3 4 0 1 1
4 1 q 1 1
4 1 a 1 2
4 1 b 1 3
4 1 c 1 4
0
\end{lstlisting}
\end{minipage}
Transcription from clingo to aspif is straightforward. Each disjunctive head
is transcribed as a single disjunctive rule.

\subsubsection{Cardinality constraints}

\begin{equation*}
    \texttt{l }\{p_1; \dots; p_m; \aspnot p_{m+1}; \dots; \aspnot p_n\}\texttt{ u}
\end{equation*}
\begin{equation*}
    \texttt{l <= }\{p_1; \dots; p_m; \aspnot p_{m+1}; \dots; \aspnot p_n\}\texttt{ <= u}
\end{equation*}
Cardinality constraint is so called conditional literal. It's semantics is that
at least $l$ and at most $u$ of literals
$p_1; \dots; p_m; \aspnot p_{m+1}; \dots; \aspnot p_n$
has to be in the answer set for this literal to hold.
Cardinality constraint can be also further unboudned on either one or both
sides. Cardinality constraints
could be encoded into ASP using oriented binary decision diagram.
% TODO: Should there be further analysis of this?
However, probably due to the complexity of this transformation size, gringo
encodes this conditional literal as (possibly) multiple rules with weight
bodies~\cite{aspEasy2016}.

The cardinality constraint can also be used without either one or both of
$l$ or $u$. That way it is unbounded from bottom or top respectively. This
is especially usefull for defining input space.

When transcripting from clingo language to aspif, each cardinality constraint
rule translates to up to two weighted body rules and up two normal rules.
When used in head, additional choice rule is used.

\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]

{a; b; c}.
q :- 1 {a; b; c} 2.
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[numbers=right, countblanklines=false]
asp 1 0 0
1 1 3 1 2 3 0 0
1 0 1 4 1 1 3 1 1 2 1 3 1
1 0 1 5 1 3 3 1 1 2 1 3 1
1 0 1 6 0 2 4 -5
1 0 1 7 0 1 6
4 1 a 1 1
4 1 b 1 2
4 1 c 1 3
4 1 q 1 7
0
\end{lstlisting}
\end{minipage}
On line 1 in clingo and 2 in aspif, solver can choose any number of atoms
\texttt{a}, \texttt{b}, \texttt{c}. Then rule on line 2 in clingo translates
into lines 3--6 in aspif. Rules on lines 3, 4 create new literals 4, 5. These
are set if lower bound is filled (sum is at least 1) or upper bound overshot
(sum is more than 2, that is at least 3). Then on line 5
a new literal 6 is set if 4 is set and 5 is not set. This corresponds
to filling the lower bound and not overshooting upper bound simultaneously.
Rule on line 6 is then used to set literal corresponding to \texttt{q}.
In this case the rule is redundant, but generally this rule would implement
the conjunction of body literals in the clingo rule.

Use of the cardinality constraint in the head is very similiar.

\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]

{q}.
1 {a; b; c} 2 :- q.
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[numbers=right, countblanklines=false]
asp 1 0 0
1 1 1 1 0 0
1 0 1 2 0 1 1
1 1 3 3 4 5 0 1 2
1 0 1 6 1 1 3 3 1 4 1 5 1
1 0 1 7 1 3 3 3 1 4 1 5 1
1 0 1 8 0 2 6 -7
1 0 0 0 2 2 -8
4 1 q 1 1
4 1 a 1 5
4 1 b 1 4
4 1 c 1 3
0
\end{lstlisting}
\end{minipage}
The main difference is in aspif file, line 8. Instead of positive building
literals as in previous example on line 6, constraint is used. Thus either the
rule is not used resulting in nonexistence of literal 2 or the literal 8
is set, corresponding to the cardinality constraint holding.
Another difference is on the line 4. In the example with cardinality constraint
in the body, the choice rule was added by another statement, while here it is
part of transcribtion of this rule.

\begin{equation*}
    \texttt{l} \diamond_1 \{p_1; \dots; p_m; \aspnot p_{m+1}; \dots; \aspnot p_n\} \diamond_2 \texttt{u}
\end{equation*}
Similiarly to usage of operator \texttt{<=} in the statement, operators
\texttt{<}, \texttt{>=}, \texttt{>}, \texttt{=}, \texttt{!=} can be used in its
place. In case of \texttt{=}, the formula is rewritten using two \texttt{<=}.
The operator \texttt{!=} is evaluated two \texttt{<=} in disjunction.

In the body of statement, the operator \texttt{=} can be also used to assign
value to variable. In that case, for each possible value, a new literal is
created which is in the answer set if the equality holds. Thus the following
two clingo programs are parsed into the same aspif file.

\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
{a; b}.
q(X) :- X = {a; b}.
\end{lstlisting}
\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
{a; b}.
q(2) :- 2 = {a; b}.
q(1) :- 1 = {a; b}.
q(0) :- 0 = {a; b}.
\end{lstlisting}
\end{minipage}
\hspace{1em}
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[numbers=right, countblanklines=false]
asp 1 0 0
1 1 2 1 2 0 0
1 0 1 3 1 1 2 1 1 2 1
1 0 1 4 0 1 -3
1 0 1 5 1 1 2 1 1 2 1
1 0 1 6 1 2 2 1 1 2 1
1 0 1 7 0 2 5 -6
1 0 1 8 0 1 7
1 0 1 9 1 2 2 1 1 2 1
1 0 1 10 0 1 9
4 1 a 1 1
4 1 b 1 2
4 4 q(0) 1 4
4 4 q(1) 1 8
4 4 q(2) 1 10
0
\end{lstlisting}
\end{minipage}

It can be seen that the generated program is highly redundant. For instance
pairs of literals \texttt{3} and \texttt{5}, \texttt{6} and \texttt{9},
\texttt{7} and \texttt{8}, \texttt{9} and \texttt{10} are the same.
This might be on purpose as it lowers the computational time of Clasp.
I have measured the time to go through all solutions of both file generated by
the gringo and equivalent file without redundancy. The files use modified
program equivalent to the one above with 20 literals.
% TODO: neni to velmi nepresne? literalu je tam vice
The results are written in table below.

{\setlength{\tabcolsep}{0.4em}%
\begin{center}
    \begin{tabular}{l @{\hskip 1em} r r}\toprule{}%
        File               & Best of 3 & Average of 3 \\\midrule{}
        With redundancy    & 0.933 s   & 0.970 s \\
        Without redundancy & 3.278 s   & 3.287 s \\
        \bottomrule{}
    \end{tabular}
\end{center}}\noindent

% TODO: Is this section needed?
% \subsubsection{Normal logic programs}
% 
% Each rule of normal logic program is represented as one of three possible forms:
% {\setlength{\tabcolsep}{0.2em}%
% \begin{center}
%     \begin{tabular}{l @{\hskip 1em} L l L @{\hskip 1em} L}\toprule{}%
%         Type & \multicolumn{3}{l}{Form} & \mathrm{ASP\ rule}\\\midrule{}%
%         Fact & A_0. & & & \asprule{A_0}{}\\
%         Rule & A_0 &:- & L_0, \dots, L_n. & \asprule{A_0}{L_0, \dots, L_n}\\
%         Constraint & &:- & L_0, \dots, L_n. & \asprule{\phantom{A_0}}{L_0, \dots, L_n}\\
%         \bottomrule{}
%     \end{tabular}
% \end{center}}\noindent
% where atom $A_0$ is rule head and literals $L_0, \dots, L_n$ are rule body.
% Semantics of these rules are the same as of corresponding ASP rules.
% If all literals $L_0, \dots, L_n$ are consistent with the answer set,
% then atom $A_0$ is in the answer set.

\subsubsection{Aggregates}

\begin{equation*}
    \texttt{l} \diamond_1 \texttt{\#agg}\{p_1; \dots; p_m; \aspnot p_{m+1}; \dots; \aspnot p_n\} \diamond_2 \texttt{u}
\end{equation*}
Similiar to cardinality constraints are the aggregates. These work as functions
applied on sets of tuples. Usable aggregate functions are \texttt{\#count},
\texttt{\#sum}, \texttt{\#sum+}, \texttt{\#min} and \texttt{\#max}.
As in cardinality constraints, both $\diamond_1, \diamond_2$ default to 
\texttt{<=} if ommited.

% TODO: examples

\subsection{Clasp}

% TODO: section

\chapter{Můj přínos}
% TODO: Rename section

\section*{Notation}

Let me first introduce you to the notation for this chapter.

\begin{table}[h]
    {%
    \makebox[\textwidth]{%
\begin{tabular}{ll}
    \toprule{}%
    \text{Symbol} & Definition \\ \midrule{}%
    $\NN_0$ & Set of nonnegative integers, that is $\{0, 1, 2, \dots\}$ \\
    $\BB$ & $\{0, 1\}$ \\
    $\BB_\pm$ & $\{-1, 1\}$ \\
    $\RR$ & Real numbes\\
    $S^k$ & Set of vectors
        $\{(s_1, s_2, \dots, s_k) \mid s_1, s_2, \dots, s_k \in S\}$ \\
    $\BNN$ & Binary network as a function\\
    $\mat{M}$ & Matrix\\
    $\vec{v}$ & Vector\\
    $\vec{1}$ & Vector of 1's\\
    $[k]$ & Set of numbers up to $k$, that is $\{1, 2, \dots, k\}$\\
    $\mat{M}_{:, j}$ & j-th row of matrix $\mat{M}$\\
    $\vec{v}_j$ & j-th entry of vector $\vec{v}$\\
    $\lfloor x \rfloor$ & bottom whole part\\
      & $\lfloor x \rfloor = y \iff x = y + q \wedge 0 \leq q < 1$
    \\ \bottomrule
\end{tabular}
    }}%
    \caption{Used notation}%
    \label{table:mat_notation}
\end{table}

\section{Binary neural network}\label{section:binneurnetw}
I have been working with deterministic binarized neural networks (BNN)
as defined in~\cite{Hubara2016BinarizedNN} and~\cite{10.1145/3563212}.
For convinience I have rewritten the output as a natural number instead of
one-hot vector.
This article defines deterministic BNN as a convolution of layers.
The input is
encoded as vector $\BB_{\pm 1}^{n_1}$. The BNN is then encoded as tuple of blocks
$(t_1, t_2, \dots, t_d, t_{d+1})$.
\begin{equation*}
    \BNN: \, \BB_{\pm 1}^{n_1} \to \NN_0
\end{equation*}
\begin{equation}
    \BNN = t_{d+1} \circ t_d \circ \dots \circ t_1,
\end{equation}
where for each $i\in \{1, 2, \dots, d\}$, $t_i$ is inner block consisting
of LIN layer $t_i^{lin}$, BN layer $t_i^{bn}$ and BIN layer $t_i^{bin}$.

\begin{equation*}
    t_i: \, \BB_{\pm 1}^{n_i} \to \BB_{\pm 1}^{n_{i+1}}
\end{equation*}
\begin{equation}
    t_i = t_i^{bin} \circ t_i^{bn} \circ t_i^{lin}
\end{equation}
Output block $t_{d+1}$ then consists of LIN layer $t_{d+1}^{lin}$ and ARGMAX layer
$t_{d+1}^{am}$.
\begin{equation*}
    t_{d+1}: \, \BB_{\pm 1}^{n_{d+1}} \to \NN_0
\end{equation*}
\begin{equation}
    t_{d+1} = t_{d+1}^{am} \circ t_{d+1}^{lin}
\end{equation}

Each layer is a function with parameters defined in Table~\ref{table:bnn_layers}.

\begin{table}[h]
    \makebox[\textwidth]{%
\begin{tabular}{lLlL}
    \toprule{}%
    Layer & \text{Function} & Parameters & \text{Definition}
    \\ \midrule{}%
    LIN & t_i^{lin}: \, \BB{}_{\pm{} 1}^{n_i} \to{} \RR{}^{n_{i+1}} &
        Weight matrix: $\mat W\in \BB_{\pm 1}^{n_i \times n_{i+1}}$ &
        t_i^{bn} (\vec{x}) = \vec{y}$, where $\forall{} j\in[n_{i+1}]
    \\
        &    & 
        Bias (row) vector: $\vec b \in \RR^{n_{i+1}}$ &
        \vec{y_j} = \langle{} \vec{x}, \mat{W}_{:, j} \rangle{} + \vec{b_j}
    \\ \addlinespace[0.5em]
    BN & t_i^{bn}: \, \RR^{n_{i+1}} \to{} \RR^{n_{i+1}} &
        Weight vector: $\vec \alpha\in \RR^{n_{i+1}}$ &
        t_i^{bn} (\vec{x}) = \vec{y}$, where $\forall{} j\in[n_{i+1}]
    \\
    & &
        Bias vector: $\vec \gamma \in \RR^{n_{i+1}}$ &
        \vec{y_j} = \vec{\alpha_j}
            \cdot \frac{\vec{x_j} \ensuremath{-} \vec{\mu_j}}{\vec{\sigma_j}}
            + \vec{\gamma_j}
    \\
    & &
        Mean vector: $\vec \mu \in \RR^{n_{i+1}}$ &
    \\
    & &
        Std.\ dev.\ vector: $\vec \sigma \in \RR^{n_{i+1}}$ &
    \\ \addlinespace[0.5em]
    BIN & t_i^{bin}: \, \RR^{n_{i+1}} \to{} \BB{}_{\pm{} 1}^{n_{i+1}} &
        \makecell[c]{-} &
        t_i^{bin} (\vec{x}) = \vec{y}$, where $\forall{} j\in[n_{i+1}],
    \\
    & & &
        \vec{y_j} = \left\{
            \begin{matrix*}[l]
                +1, & \mathrm{if} \, \vec{x_j} \geq{} 0; \\
                -1, & \mathrm{otherwise}
            \end{matrix*}
            \right.
    \\ \addlinespace[0.5em]
    ARGMAX & t_{d+1}^{am}: \, \RR^{n_{d+1}} \to{} \NN_0 &
        \makecell[c]{-} &
    \makecell[l]{%
        t_{d+1}^{am} (\vec{x}) = \arg{} \max{} (\vec{x})
        % TODO if multiple argmax is minimal index
    }
    \\ \bottomrule
\end{tabular}
    }%
    \caption{Definition of BNN layers~\cite{10.1145/3563212}}%
    \label{table:bnn_layers}
\end{table}

I have transformed the parameters to lower their count and make them integer only.

\begin{equation*}
    t_i(\vec x) = (t_i^{bin} \circ t_i^{bn} \circ t_i^{lin})(\vec x) = y
\end{equation*}
\begin{equation*}
    y_j = \sign_{\pm 1}\left( \alpha_j \cdot \frac{\langle \vec x, \mat W_{:, j}\rangle + b_j - \vec \mu_j}{\vec \sigma_j} + \vec \gamma_j \right)
\end{equation*}

The argument of $\sign_{\pm 1}$ can be further analysed.

\begin{equation*}
    \vec \alpha_j \cdot \frac{\langle \vec x, \mat W_{:, j}\rangle + \vec b_j - \vec \mu_j}{\vec \sigma_j} + \vec \gamma_j \geq 0
\end{equation*}

There are three possible cases of value $\frac{\vec \alpha_j}{\vec \sigma_j}$:

\begin{equation*}
    \begin{matrix}
        \frac{\vec\alpha_j}{\vec\sigma_j} > 0: \\
        \langle \vec x, \mat W_{:, j}\rangle + \vec b_j - \vec \mu_j + \frac{\vec \sigma_j}{\vec\alpha_j}\cdot \vec \gamma_j \geq 0\\
        \mat W'_{:, j} = \mat W_{:, j}, \,
        \vec b'_j = \vec b_j - \vec\mu_j + \frac{\vec\sigma_j}{\vec\alpha_j}\cdot \vec\gamma_j\\
        \langle \vec x, \mat W'_{:, j}\rangle + \vec b'_j \geq 0\\
    \end{matrix}
\end{equation*}

\begin{equation*}
    \begin{matrix}
        \frac{\vec\alpha_j}{\vec\sigma_j} < 0: \\
        \langle \vec x, \mat W_{:, j}\rangle + \vec b_j - \vec \mu_j + \frac{\vec \sigma_j}{\vec\alpha_j}\cdot \vec \gamma_j \leq 0 \\
        \langle \vec x, -\mat W_{:, j}\rangle - \vec b_j + \vec \mu_j - \frac{\vec \sigma_j}{\vec\alpha_j}\cdot \vec \gamma_j \geq 0\\
        \mat W'_{:, j} = -\mat W_{:, j}, \,
        \vec b'_j = -\vec b_j + \vec\mu_j - \frac{\vec\sigma_j}{\vec\alpha_j}\cdot \vec\gamma_j\\
        \langle \vec x, \mat W'_{:, j}\rangle + \vec b'_j \geq 0\\
    \end{matrix}
\end{equation*}

\begin{equation*}
    \begin{matrix}
        \vec\alpha_j = 0: \\
        \vec\gamma_j \geq 0\\
        \mat W'_{:, j} = \vec 0, \,
        \vec b'_j = \vec\gamma_j\\
        \langle \vec x, \mat W'_{:, j}\rangle + \vec b'_j \geq 0\\
    \end{matrix}
\end{equation*}

This shows that each inner layer can be computed using weight matrix and
one bias parameter. However if there were some parameter $\alpha_j = 0$,
$\mat W'$ would not have values only from $\pm 1$ but from $\{-1, 0, 1\}$.
This is not an issue as Clingo works by default on integers.
If 0-values were issue, they could be removed either by offsetting bias
parameter or by modifying the BNN structure to remove the constant node.
(This has not been implemented in my work.)

\begin{equation*}
    \vec \alpha_j = 0:\, W'_{:,j} = W_{:,j},\, \vec b'_j = \left\{
        \begin{matrix}
            \langle\vec 1, \vec 1\rangle + 1 & \mathrm{if}\, \vec\gamma_j \geq 0;\\
            -\langle\vec 1, \vec 1\rangle - 1 & \mathrm{otherwise}
        \end{matrix}
    \right.
\end{equation*}

In my implementation I am also encoding inputs as binary values $\{0, 1\}$.

\begin{equation*}
    \begin{matrix}
        \vec x = 2 \vec x^{(b)} - \vec1\\
        \langle \vec x, \mat W'_{:, j}\rangle + \vec b'_j \geq 0\\
        \langle 2\vec x^{(b)} - \vec1, \mat W'_{:, j}\rangle + \vec b'_j \geq 0\\
        2\langle \vec x^{(b)}, \mat W'_{:, j}\rangle - \langle\vec1, \mat W'_{:,j}\rangle + \vec b'_j \geq 0\\
        \langle \vec x^{(b)}, \mat W'_{:, j}\rangle + \frac{- \langle\vec1, \mat W'_{:,j}\rangle + \vec b'_j}{2} \geq 0\\
    \end{matrix}
\end{equation*}

Still each block could be computed using one weight matrix and one bias vector.
Additionally, as $\langle\vec x^{(b)}, \mat W'_{:, j}\rangle$ is an integer value,
equation is equivalent to

\begin{equation}
    \langle \vec x^{(b)}, \mat W'_{:, j}\rangle + \left\lfloor \frac{- \langle\vec1, \mat W'_{:,j}\rangle + \vec b'_j}{2} \right\rfloor \geq 0\\
    \label{eq:layer_comp}
\end{equation}

As for the output block, it has already just one weight matrix and one bias
vector. The implementation of ARGMAX layer is mainly Clingo-based.
The only needed transformation is to split bias to whole and decimal part.

\begin{equation*}
    \vec b = \vec b' + \vec p, \, \vec b'_j = \lfloor \vec b_j \rfloor
\end{equation*}

$\vec p$ is then used as main order of outputs.
% TODO: Try to order the outputs by this order? -> would remove one aggregate

Implementation of these transformations in Python is added as [appendix] % TODO

\section{Encoding of BNN in Clingo}

Input for the Clingo was parsed from the model using Python program. This
program also computed the transformation from Section~\ref{section:binneurnetw}.
This program then outputs the model of BNN as multiple rules.
Semantics are as follows.

% TODO: difference between layer and block
\fact{layer}{$L, N$}
Layer $L$ consists of $N$ nodes. Layer 0 is input layer, thus its number
of nodes corresponds to the length of input vector.
Highest layer is the output layer.

\fact{weight}{$L, P, N, W$}
Weight from node $P$ of layer $L-1$ to node $N$ of layer $L$ is $W$.

\fact{bias}{$L, N, B$}
Bias of node $N$ of layer $L$ is $B$.

\fact{outpre}{$N, P$}
Output node $N$ has a precedence of $P$. If tied, the output
with higher precedence is the result.

The encoding of input region is written in Section~\ref{section:inputregion}.

I have transcribed the computation of BNN as the following clingo program.

\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
% output layer
output_layer(L) :- L = #max{ X : layer(X, _) }.

% any combination of input bits is possible
{on(0, 0..K-1)} :- layer(0, K).

% hidden layers
on(L, N) :-
    #sum{ W,I : on(L-1, I), weight(L, I, N, W) } >= B,
    bias(L, N, B), not output_layer(L).

% output layer before arg max layer
outnode(N, S + B) :-
    S = #sum{ W,I : on(L-1, I), weight(L, I, N, W) },
    output_layer(L), bias(L, N, B).

% output the node with highest sum
% if tied on sum, then with highest precedence
% if tied on both sum and precedence, then with lowest number
output(Node) :-
    (Sum, Order, -Node) = #max{ (S, O, -N) : outnode(N, S), outpre(N, O) }.
\end{lstlisting}

On line 2, it computes the output layer. Line 4 allows for arbitrary assignment
of input vector. Then next layers are computed on lines 6--8. This computation
follows the Equation~\ref{eq:layer_comp}.

Computation of the output is performed in two steps. First, on lines 10--12,
sum of weighted inputs and bias is computed into the \texttt{outnode}, then
on lines 16--17, the maximal of these values is choosen to be the output.

Grounding of most of this program is straightforward. Line 2 expands to
a single fact. Line 4 expands to number of choice rules equal to size of input
vector. Lines 6--8 expand to a number of rules equal to hidden nodes of the
network.

Encoding of the output layer in this case is problematic. Rule
on lines 10--12 generate for each of the output bits one more atom than is
the size of last hidden layer, each with equality constraint. Then the rule
on lines 16--17 generate % TODO? Is this true?
exponentaly many rules for \texttt{output}.
% TODO: Is it trying all combinations e.g. also the imposible?

To mitigate this issue I have created an alternative way for computation of
the output. I have used the choice rule while constraining the output to
be the maximal. This way I have removed the need to use an intermediate step
of \texttt{outnode}. The following lines replace lines 9--17.

\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false, firstnumber=9]
% there is single output
1 {output(0..N-1)} 1 :- layer(L, N), output_layer(L).

% the sum of another is not higher
:- output(N), O = 1..M-1, output_layer(L), layer(L, M), O != N,
    #sum{ W, I, this  : on(L-1, I), weight(L, I, N, W);
         -W, I, other : on(L-1, I), weight(L, I, O, W) } < BO - BN,
    bias(L, N, BN), bias(L, O, BO).

% the sum of another is not same or order is not higher
:- output(N), O = 1..M-1, output_layer(L), layer(L, M), O != N,
    #sum{ W, I, this  : on(L-1, I), weight(L, I, N, W);
         -W, I, other : on(L-1, I), weight(L, I, O, W) } = BO - BN,
    bias(L, N, BN), bias(L, O, BO),
    outpre(N, PN), outpre(O, PO), PO > PN.
\end{lstlisting}

The grounding generates only a single atom for each output and then for each
of them a finitely many constraints. Line 10 asserts there is exactly single
output. Then the two constraints assure that there is no other output
that would have higher biased sum of previous layer and if equal, the one with
higher precedence is used. There is no need for the evaluation of output number
as the precedence already is nonredundant.

The use of choice rule and constraints has greatly reduced both size of
the aspif file generated by gringo and the total time needed for computation
of answer sets as shown in the table below.

% TODO: create table / graph for size of gringo grounded size and time
%       based on different input models

\section{Encoding of input regions}\label{section:inputregion}

% TODO: To be implemented
% Definice input region
An input region of BNN is some subset of inputs on which the analysis is
performed. I have implemented both of input region types from~\cite{10.1145/3563212},
that is input region based on Hamming distance ($R_H$) and input region with
fixed indices ($R_I$).

The quantitative analysis of said BNN is a problem to say how many inputs
in the input region differ from ground truth (or in this case the base input)
in the outputs.

To perform the quantitative analysis on some input region the encoding of such
region is needed.

\subsection{Hamming distance}

\begin{equation*}
    R_H(\vec u, r) = \{\vec v \mid \vec u, \vec v \in \BB^n,
                       \|\{i \mid \vec u_i \neq \vec v_i\}\| \leq r\}
\end{equation*}

Space of inputs under hamming distance $r\in\NN_0$ from input vector
$\vec u\in\BB^n$,
$R_H(\vec u, r)$ is a set of input vectors, that differs on at most $r$ entries
from the vector $\vec u$.

In my implementation, full input vector is parsed by the Python parser.
If the entry of vector represents active state, fact \texttt{input($a$).},
where $a$ corresponds to the index of that entry,
is included in the model. Else nothing is added to the model. The maximal
allowed hamming distance is represented as fact \texttt{hamdist($r$).},
where $r$ corresponds to the allowed radius of this distance.

\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
% input space is at most hamdist from on
:- #count{ N : not input(N), on(0, N); N : input(N), not on(0, N) } > H,
    hamdist(H).
\end{lstlisting}
The input region is then encoded in the Clingo language as a single constraint.

\subsection{Fixed indices}

\begin{equation*}
    R_I(\vec u, I) = \{\vec v\in \BB^n \mid \forall i\in I \,.\, \vec u_i = \vec v_i\}
\end{equation*}

Input region $R_I(\vec u, I)$ given by fixed indices is
a set of input vectors that do not
differ from the base vector $\vec u \in \BB^{n_1}$ on entries
on indices from $I \subseteq [n_1]$.

Like in the input region based of hamming distance, the full vector $\vec u$
is added to the model using facts \texttt{input($a$).}. Then, the fixed indices
in the form of fact \texttt{inpfix($i$).} for every index $i\in I$ is added.
Note that this defintion is dual to the one in~\cite{10.1145/3563212}.

\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
% input does not differ from base on fixed indices
:- inpfix(N), on(0, N), not input(N).
:- inpfix(N), not on(0, N), input(N).
\end{lstlisting}
The encoding into Clingo is again simple. If the index is fixed, then the
input \texttt{on(0, N)} and base input \texttt{input(N)} can not differ.

\subsection{Encoding of adversarial output}

For the output to differ from the one of base input, the program has to be
further constrained. To implement the feature for the output to be adversarial,
I have first used the same encoding as for the computing of the output of the
network.
% TODO: The direct constraint for the output to not be the one given
%       by the base input is faster and smaller?

\begin{lstlisting}[language=prolog, numbers=left, countblanklines=false]
% Show only inputs with output nonequal to that of input vector
inputOn(0, N) :- input(N).
inputOn(L, N) :-
    #sum{ W,I : inputOn(L-1, I), weight(L, I, N, W) } >= B,
    bias(L, N, B), not output_layer(L).

1 {inputOutput(0..N-1)} 1 :- layer(L, N), output_layer(L).

% the sum of another is not higher
:- inputOutput(N), O = 1..M-1, output_layer(L), layer(L, M), O != N,
    #sum{ W, I, this  : inputOn(L-1, I), weight(L, I, N, W);
         -W, I, other : inputOn(L-1, I), weight(L, I, O, W) } < BO - BN,
    bias(L, N, BN), bias(L, O, BO).
% the sum of another is not same or order is not higher
:- inputOutput(N), O = 1..M-1, output_layer(L), layer(L, M), O != N,
    #sum{ W, I, this  : inputOn(L-1, I), weight(L, I, N, W);
         -W, I, other : inputOn(L-1, I), weight(L, I, O, W) } = BO - BN,
    bias(L, N, BN), bias(L, O, BO),
    outpre(N, PN), outpre(O, PO), PO > PN.

:- output(Node), inputOutput(Node).
\end{lstlisting}

%As the computation of the desired input can be done in polynomial time,
%in the end I have computed the output using Python while generating
%the input file for Clingo. Thus the constraint on the output is included
%in the data for ASP computation.
%Or maybe not, it is slower in the end ‾\_O_/‾

% TODO: possibility of encoding BNN with rational weights?

\chapter{Evaluation}

\section{Methodology of evaluation}

For the evaluation of my model, I have used inputs from dataset
% TODO: is this true?
of BNNQuanalyst. In the next chapter I will be reffering to instances of inputs
0 to 9 as I0 to I9. The instances I have used for the testing are shown
on Figure [].
% TODO: name them I0 - I9
% TODO: link
% TODO: pictures
The architectures of different models is written in Table~\ref{table:model_architecture}

\begin{table}
\begin{tabular}{l c | l c}  % chktex 44
    \toprule{}%
    Model & Architecture  & Model & Architecture       \\ \midrule
    M1    & 100:100:10    & M7    & 100:50:20:10       \\
    M2    & 100:50:10     & M8    & 16:25:20:10        \\
    M3    & 400:100:10    & M9    & 36:15:10:10        \\
    M4    & 64:10:10      & M10   & 16:64:32:20:10     \\
    M5    & 784:100:10    & M11   & 25:25:25:20:10     \\
    M6    & 100:100:50:10 & M12   & 784:50:50:50:50:10 \\ \bottomrule
\end{tabular}
\caption{Architectures of models}%
    \caption*{Values are numbers of neurons in each layer}%
    \label{table:model_architecture}
\end{table}


The framework was evaluated using the computational sources of MetaVO,
virtual organization providing computing resources for academic community.
The framework was evaluated on hardware
[TODO]  % TODO: actuall hardware of the run
The framework was evaluated on 8 threads with 16 GB of RAM unless otherwise  % chktex 13
specified.

%\section{Evaluation of different solvers}
%I have evaluated BNN models 
% TODO: if time will suffice

\section{Evaluation over different inputs}

I have evaluated BNN models M1, M2, M6, M7 agnist all inputs I0-I9 with
hamming distance 3. The results are shown in the Table~\ref{table:eval_inputs}.
As can be seen, the time to find all adversary inputs is independent
on the number of all adversary inputs.
Time to find 1st model (adversary input) is dependent on the number
of models (adversary inputs). The more models exist for the problem, the higher
chance is to find one by assigning values for literals at random. The table
also shows that the size of aspif intermediate file does not change much
between different inputs. The quantitative verification using ASP
seems to scale better with depth of the evaluated BNN than with width.

\landtable{
    \centering % Center table
    \begin{tabular}{r | R R R R R | R R R R R}  % chktex 44
        \toprule{}%
        % TODO: Refresh new values
        Input & \#M & T_M & T_1 & N_r & N_w & \#M & T_M & T_1 & N_r & N_l \\ \midrule
        & &&\multicolumn{1}{r}{M1}&& & &&\multicolumn{1}{r}{M2}&& \\ \midrule
        I0 & 40821 & 34.143 & 0.14 & 11151 & 44386 & 16403 & 17.822 & 0.20 & 5601 & 22236 \\
        I1 & 10945 & 39.961 & 0.12 & 11138 & 44373 & 34288 & 16.690 & 0.03 & 5588 & 22223 \\
        I2 & 22518 & 39.480 & 0.14 & 11154 & 44389 & 26628 & 15.254 & 0.07 & 5604 & 22239 \\
        I3 & 20666 & 37.030 & 0.13 & 11152 & 44387 & 10955 & 14.008 & 0.12 & 5602 & 22237 \\
        I4 &114392 & 31.222 & 0.09 & 11149 & 44384 & 70244 & 14.608 & 0.06 & 5599 & 22234 \\
        I5 & 36405 & 32.362 & 0.14 & 11147 & 44382 &140021 & 12.831 & 0.03 & 5597 & 22232 \\
        I6 &     0 & 31.798 &    - & 11153 & 44388 &   487 & 18.832 & 0.20 & 5603 & 22238 \\ %chktex 8
        I7 & 53242 & 32.387 & 0.07 & 11144 & 44379 & 15055 & 17.794 & 0.07 & 5594 & 22229 \\
        I8 &139399 & 41.975 & 0.09 & 11151 & 44386 & 47558 & 14.506 & 0.07 & 5601 & 22236 \\
        I9 & 42591 & 32.511 & 0.11 & 11150 & 44385 & 74443 & 13.078 & 0.04 & 5600 & 22235 \\ \midrule
        & &&\multicolumn{1}{r}{M6}&& & &&\multicolumn{1}{r}{M7}&& \\ \midrule
        I0 & 51423 & 79.991 & 0.94 & 15702 & 62538 &  3330 & 27.282 & 0.05 & 6322 & 25098 \\
        I1 &   196 &141.390 & 2.89 & 15689 & 62525 & 35548 & 19.151 & 0.12 & 6309 & 25085 \\
        I2 & 45315 & 84.534 & 0.95 & 15705 & 62541 & 20752 & 17.112 & 0.09 & 6325 & 25101 \\
        I3 &  6725 &104.792 & 1.09 & 15703 & 62539 &  8238 & 25.747 & 0.12 & 6323 & 25099 \\
        I4 & 24909 &124.571 & 1.25 & 15700 & 62536 & 74224 & 18.924 & 0.04 & 6320 & 25096 \\
        I5 & 22733 & 72.106 & 1.29 & 15698 & 62634 & 11053 & 20.125 & 0.11 & 6318 & 25094 \\
        I6 &  6283 &128.535 & 1.35 & 15704 & 62540 & 15919 & 23.074 & 0.14 & 6324 & 25100 \\
        I7 &  7091 &104.315 & 1.63 & 15695 & 62531 &  8361 & 21.750 & 0.15 & 6315 & 25091 \\
        I8 & 49700 & 86.873 & 1.39 & 15702 & 62538 &146724 & 21.501 & 0.04 & 6322 & 25098 \\
        I9 & 80704 &170.094 & 1.13 & 15701 & 62537 & 24300 & 20.277 & 0.13 & 6321 & 25097 \\ \bottomrule
    \end{tabular}
    \caption{Evaluation of M1, M2, M6, M7 over inputs I0 --- I9}%
    \caption*{\centering%
        $\#M$ number of adv.\ inputs,
        $T_M$ time to all adv.\ inputs [sec], $T_1$ time to 1st adv.\ input [sec]
        \\
        $N_r$ number of lines (approx.\ rules) of aspif,
        $N_l$ number of words (approx.\ literals) of aspif}%
    \label{table:eval_inputs}
}
% TODO: maybe remake into graph?

\section{Evaluation over different hamming distance}

I have evaluated models M2, M3, M7, M11 on input I0 for hamming distance
from 0 to 4. The results are shown in the Table~\ref{table:eval_distance}.
As can be seen, the hamming distance does not affect the size of the aspif
intermediate file at all. This is because the hamming distance does only change
weight in a single constraint corresponding to the encoding of input region
by hamming distance.
The time to find all models (adversarial inputs) scales badly with hamming
distance for big input layers. This is expected as the input space
of inputs under hamming distance grows for small distances close
to exponentially with size of the input layer in the base. The effect
of size of the base is apparent in the evaluation of model M11,
here the time grows relatively slowly.

\landtable{
    \centering % Center table
    \begin{tabular}{r | R R R R R | R R R R R}  % chktex 44
        \toprule{}%
        % TODO: Refresh new values
        Distance & \#M & T_M & T_1 & N_r & N_w & \#M & T_M & T_1 & N_r & N_l \\ \midrule
        & &&\multicolumn{1}{r}{M2}&& & &&\multicolumn{1}{r}{M3}&& \\ \midrule
        0 & 0           &        0.079 &    - & 5601 & 22236 & 0      & 0.476        & - & 41205 & 164440 \\ % chktex 8
        1 & 0           &        0.141 &    - & 5601 & 22236 & 0      & 1.717        & - & 41205 & 164440 \\ % chktex 8
        2 & 91          &        0.707 & 0.05 & 5601 & 22236 & 0      & 146.393      & - & 41205 & 164440 \\ % chktex 8
        3 & 16404       &       28.721 & 0.17 & 5601 & 22236 &[TO]\,0 &[TO]\,300.050 & - & 41205 & 164440 \\ % chktex 8 % TODO: is this right?
        4 &[TO]\,507959 &[TO]\,300.006 & 0.04 & 5601 & 22236 &[TO]\,0 &[TO]\,300.299 & - & 41205 & 164440 \\ \midrule % chktex 8
        & &&\multicolumn{1}{r}{M7}&& & &&\multicolumn{1}{r}{M11}&& \\ \midrule
        0 & 0           &   0.105      &    - & 6322 & 25098 &     0 & 0.054 &    - & 2052 & 8079 \\ % chktex 8
        1 & 0           &   0.258      &    - & 6322 & 25098 &    12 & 0.157 & 0.05 & 2052 & 8079 \\ % chktex 8
        2 & 73          &   1.122      & 0.14 & 6322 & 25098 &   207 & 0.125 & 0.05 & 2052 & 8079 \\
        3 & 3330        &  23.355      & 0.11 & 6322 & 25098 &  1813 & 0.237 & 0.05 & 2052 & 8079 \\
        4 &[TO]\,191969 &[TO]\,300.021 & 0.11 & 6322 & 25098 & 11830 & 0.950 & 0.05 & 2052 & 8079 \\ \bottomrule
    \end{tabular}
    \caption{Evaluation of M2, M3, M7, M11 over hamming distance 0~---~4}%
    \caption*{\centering
        $\#M$ number of adv.\ inputs,
        $T_M$ time to all adv.\ inputs [sec], $T_1$ time to 1st adv.\ input [sec],
        \\
        $N_l$ number of lines (approx.\ rules) of aspif,
        $N_w$ number of words (approx.\ literals) of aspif
        \\
        $[TO]$ solving has timed out}%
    \label{table:eval_distance}
}
% TODO: maybe remake into graph?

\section{Evaluation over fixed input bits}

Fixed bits vectors were created from the base input instances corresponding
to the number 0 using
the following bash script. The script creates for each input instance
4 fixed bits vectors by removing the first 0, 8, 16, resp. 24 positions from
the list of fixed bits.
\begin{lstlisting}[language=bash, numbers=left, countblanklines=false, escapeinside=``]
for l in 25 100 400; do
  for pos in $(eval echo {0..$(( $l-1 ))}); do
    echo -n "$pos "
  done | head --bytes=-1 "inpbits_0_${l}_0.txt"
  for free in 8 16 24; do
    cat "inpbits_0_${l}_$(( $free-8 )).txt" \
    | cut -d' ' -f'9-' > "inpbits_0_${l}_${free}.txt"
  done
done
\end{lstlisting}

For created fixed bits vectors models M2, M3, M7, M11 were evaluated on instance I0.
Results are included in the Table~\ref{table:eval_inpbits}.
This task seems to be better suited for my framework as the solver, Clasp, can prune
large subspaces of the input space at once. In the model M3, where only a small
subspace of the input space is adversarial for this input, the framework is
significantly faster than on the other models. On the others --- M2, M7 and M11 ---
Clasp probably spends most of the computation time on the enumeration
of models (adversarial inputs). As far as I am aware, Clasp is unable to count
models any other way. If it could count them in batches like it can prune
sets, where it knows no models are present, the evaluation of models M2, M7 and M11 might
be speeded up significantly.

% TODO: better comparison of the hamm dist vs fixed bits

\landtable{
    \centering % Center table
    \begin{tabular}{r | R R R R R | R R R R R}  % chktex 44
        \toprule{}%
        % TODO: Refresh new values
        Free & \#M & T_M & T_1 & N_r & N_w & \#M & T_M & T_1 & N_r & N_l \\ \midrule
        & &&\multicolumn{1}{r}{M2}&& & &&\multicolumn{1}{r}{M3}&& \\ \midrule
        0  &        0 &   0.079 &    - & 5700 & 22335 &    0 & 0.443 &    - & 41604 & 164839 \\ % chktex 8
        8  &      215 &   0.103 & 0.01 & 5692 & 22327 &    0 & 0.446 &    - & 41596 & 164831 \\ % chktex 8
        16 &    50517 &   0.484 & 0.01 & 5684 & 22319 &    0 & 0.486 &    - & 41588 & 164823 \\ % chktex 8
        24 & 12467600 & 125.292 & 0.01 & 5676 & 22311 & 2704 & 1.705 & 0.05 & 41580 & 164815 \\ \bottomrule
        & &&\multicolumn{1}{r}{M7}&& & &&\multicolumn{1}{r}{M11}&& \\ \midrule
        0  &        0 &   0.096 &    - & 6421 & 25197 &             0 &         0.053 &    - & 2076 & 8103 \\ % chktex 8
        8  &       58 &   0.154 & 0.03 & 6413 & 25189 &           226 &         0.104 & 0.05 & 2068 & 8095 \\
        16 &    38187 &   1.104 & 0.06 & 6405 & 25181 &         60144 &         2.061 & 0.07 & 2060 & 8087 \\
        24 & 11819735 & 279.607 & 0.11 & 6397 & 25183 & [TO]\,7939149 & [TO]\,300.000 & 0.07 & 2052 & 8079 \\ \bottomrule
    \end{tabular}
    \caption{Evaluation of M2, M3, M7, M11 over 0~---~24 free bit positions}%
    \caption*{\centering
        $\#M$ number of adv.\ inputs,
        $T_M$ time to all adv.\ inputs [sec], $T_1$ time to 1st adv.\ input [sec],
        \\
        $N_l$ number of lines (approx.\ rules) of aspif,
        $N_w$ number of words (approx.\ literals) of aspif
        \\
        $[TO]$ solving has timed out}%
    \label{table:eval_inpbits}
}
% TODO: hamming vs inpbits graph - time based on size of input space


% - ohodnocení modelu
% 	+ je rozdíl mezi hledání, které vrátí málo výsledků vůči hodně výsledkům
% 	+ time to first / time to all
% 	+ clingo je (asi) optimalizované na hledání jednoho řešení, tady je chceme všechny
% 	+ -> Je rozumné hledat od nejvíce svázaného zadání a postupně rozšiřovat?
% - Jaký je rozumný limit pro velikost verifikované sítě
% 	+ je problém spíše s širokými/hlubokými sítěmi
% - jak si vede vůči původnímu zdroji, dalším implementacím
% - TODO: asi to vrazit na Metacentrum či tak něco? - můj ntb by pravděpodobně zkresloval výsledky

% TODO: Nějaké readme jak to nastavit aby to fungovalo
%       toto na gitový repozitář



\chapter{Conclussion}

In this thesis I have created and implemented framework for the quantitative
analysis of binary neural networks using the answer set paradigm. My framework
uses Python to parse the neural network into Clingo-readable format and then
ASP-paradigm-based framework Clingo, combination of parser Gringo and solver
Clasp, to find adversarial inputs. In this framework I have implemented
functionality for representing input regions based on Hamming distance
and input regions based on fixed indices.

I have evaluated the framework on binary neural networks trained on the MNIST
database of handwritten digits. The framework scales very well when evaluating
robust neural networks with input region constrained by the fixed indices.
When many adversarial inputs are present, the limiting factor proved to be
the Clasp as it relies on enumeration of the models (adversarial inputs).
To eliminate this shortcomming, another solver will have to be used.
When evaluating input regions based on hamming distance, the framework has shown
better performance on networks with smaller input layer.

Answer set programming has the potential to make verification of neural networks
both easier and faster. ASP paradigm has high expressive power while
its solvers have strong heuristics to find a fast way for the computation.


% TODO: citations

% - má ASP pro další verifikaci význam?
% - je možnost, že by přinesl alespoň relevantní benchmark pro další verifikační algoritmy (např. nad obecnými NN?)
% 	+ není velká možnost, že by porazil algoritmy přizpůsobené přímo pro daný problém, ale je tu možnost že by byl fajn dokud nebudeme mít něco silnějšího než ASP pro problém

\end{document}
